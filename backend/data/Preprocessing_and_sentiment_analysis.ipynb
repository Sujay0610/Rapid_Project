{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d962405",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install xformers\n",
    "!pip install yahooquery\n",
    "!pip install yfinance\n",
    "!pip install yahoofinancials\n",
    "!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aae4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import NaN\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "import re\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, LSTM, Attention, Dropout, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "from transformers import TFRobertaModel\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from textblob import TextBlob\n",
    "\n",
    "from yahoofinancials import YahooFinancials\n",
    "from google.colab import drive\n",
    "\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8d80fe",
   "metadata": {},
   "source": [
    "## **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea7906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # remove punctuation and special characters\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # convert to lowercase\n",
    "        text = text.lower()\n",
    "        # tokenize text\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        # remove stop words\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        # lemmatize text\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        # join tokens back into text\n",
    "        text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "file_path = 'dataset_50-person-from-2021-02-05_2023-06-12_21-34-17-266.csv'\n",
    "# Load the Excel file into a DataFrame\n",
    "df = pd.read_csv(file_path,encoding='ISO-8859-1')\n",
    "\n",
    "# Remove rows with \"na\" values\n",
    "df = df.dropna(subset=['full_text'])\n",
    "\n",
    "# Fill missing values in 'full_text' column with an empty string\n",
    "df['full_text'] = df['full_text'].fillna('')\n",
    "# to lower text\n",
    "df['full_text'] = df['full_text'].str.lower()\n",
    "# Preprocess the 'full_text' column\n",
    "df['clean_text'] = df['full_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b168810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to keep rows where \"created_at\" is greater than or equal to 2021-01-01\n",
    "df = df[df['created_at'] >= '2021-01-01']\n",
    "import datetime\n",
    "# Convert the \"created_at\" column to datetime format\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "# Extract the date part from the datetime and convert it to the desired format\n",
    "df['created_at'] = df['created_at'].dt.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c49afa8",
   "metadata": {},
   "source": [
    "#### **add importance_coefficient per tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57969cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['importance_coefficient'] = df['retweet_count'] + 2 * df['favorite_count'] + 0.5 * df['reply_count']\n",
    "# Find the minimum and maximum values of the importance coefficient\n",
    "min_value = df['importance_coefficient'].min()\n",
    "max_value = df['importance_coefficient'].max()\n",
    "\n",
    "# Normalize the importance coefficient\n",
    "df['importance_coefficient_normalized'] = (df['importance_coefficient'] - min_value) / (max_value - min_value)\n",
    "# Sort the DataFrame based on the \"created_at\" column in ascending order\n",
    "df = df.sort_values('created_at', ascending=True)\n",
    "\n",
    "# Print the sorted DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dc91d5",
   "metadata": {},
   "source": [
    "## Model 1: Aspect based sentiment analysis (RoBERTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64fa0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"Your_Token_in_hugging_face\"\n",
    "# Function to extract aspects and sentiments\n",
    "def extract_aspects_sentiments(text):\n",
    "    # Load the spaCy English model for aspect extraction\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Extract aspects from the text\n",
    "    aspects = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "            aspects.append(token.text)\n",
    "\n",
    "    # Load the sentiment analysis model with your Hugging Face API token\n",
    "    sentiment_model = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\",\n",
    "        tokenizer=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\",\n",
    "        use_auth_token=access_token\n",
    "    )\n",
    "\n",
    "    # Extract sentiment for the entire text\n",
    "    sentiment_result = sentiment_model(text)[0]\n",
    "    overall_sentiment = sentiment_result[\"label\"]\n",
    "    overall_score = sentiment_result[\"score\"]\n",
    "\n",
    "    # Extract sentiment for each aspect\n",
    "    aspect_sentiments = []\n",
    "    for aspect in aspects:\n",
    "        aspect_text = text.replace(aspect, \"<aspect>\")\n",
    "        aspect_sentiment_result = sentiment_model(aspect_text)[0]\n",
    "        aspect_sentiment = aspect_sentiment_result[\"label\"]\n",
    "        aspect_score = aspect_sentiment_result[\"score\"]\n",
    "        aspect_sentiments.append((aspect, aspect_sentiment, aspect_score))\n",
    "\n",
    "    return overall_sentiment, overall_score, aspect_sentiments\n",
    "\n",
    "# Truncate the text to a maximum sequence length of 512 tokens\n",
    "df1['truncated_text'] = df1['clean_text'].str[:512]\n",
    "\n",
    "# Apply the extraction function to each row\n",
    "df1[['overall_sentiment', 'overall_score', 'aspect_sentiments']] = df1['truncated_text'].apply(extract_aspects_sentiments).apply(pd.Series)\n",
    "\n",
    "# Save the DataFrame back to CSV\n",
    "df1.to_csv('c1_c50_total_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bae1c45",
   "metadata": {},
   "source": [
    "## Model 2: RoBERTa+BiGRU an attention layer sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093ae4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "\n",
    "# split data into input and target variables\n",
    "X = df1['clean_text'].values\n",
    "y = pd.get_dummies(df1['polarity']).values\n",
    "\n",
    "# Calculate the maximum sequence length from the input data\n",
    "max_length = max([len(x.split()) for x in X])\n",
    "print(f\"Max length: {max_length}\")\n",
    "\n",
    "# split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# create Roberta tokenizer and encode inputs\n",
    "tokenizer = RobertaTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base')\n",
    "train_encodings = tokenizer(X_train.tolist(), max_length=max_length, padding=True, truncation=True, return_tensors='np')\n",
    "val_encodings = tokenizer(X_val.tolist(), max_length=max_length, padding=True, truncation=True, return_tensors='np')\n",
    "\n",
    "# define early stopping and model checkpoint\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model_res_2015.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_sentence': train_encodings['input_ids'], 'input_mask': train_encodings['attention_mask']},\n",
    "    y_train\n",
    ")).batch(32).prefetch(1)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_sentence': val_encodings['input_ids'], 'input_mask': val_encodings['attention_mask']},\n",
    "    y_val\n",
    ")).batch(32).prefetch(1)\n",
    "\n",
    "# define input layer with correct name and shape\n",
    "inputs = {\n",
    "    'input_sentence': tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_sentence'),\n",
    "    'input_mask': tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_mask')\n",
    "}\n",
    "\n",
    "# define model architecture\n",
    "roberta_model = TFRobertaModel.from_pretrained('cardiffnlp/twitter-roberta-base')\n",
    "roberta_embeddings = roberta_model({'input_ids': inputs['input_sentence'], 'attention_mask': inputs['input_mask']})[0]\n",
    "roberta_embeddings = tf.keras.layers.Dropout(0.2)(roberta_embeddings)\n",
    "gru_output = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=False))(roberta_embeddings)\n",
    "gru_output = tf.keras.layers.Dropout(0.2)(gru_output)\n",
    "attention_output = tf.keras.layers.Attention()([gru_output, gru_output])\n",
    "output = tf.keras.layers.Dense(3, activation='softmax')(attention_output)\n",
    "model_ro_res_2015 = tf.keras.models.Model(inputs=inputs, outputs=output)\n",
    "model_ro_res_2015.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train model with early stopping and model checkpoint\n",
    "history = model_ro_res_2015.fit(train_dataset, epochs=50, validation_data=val_dataset, callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7bdcda",
   "metadata": {},
   "source": [
    "## Model 3: Vader Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c63373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4d3dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected =df.copy()\n",
    "df_selected['scores'] = df_selected['full_text'].apply(lambda Description: sid.polarity_scores(Description))\n",
    "df_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3ae8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df_selected.loc[:, ['created_at', 'favorite_count', 'full_text', 'reply_count', 'retweet_count',\n",
    "                    'user/screen_name', 'clean_text', 'importance_coefficient','importance_coefficient_normalized',\n",
    "                'new_coins', 'scores']]\n",
    "df_selected = df_selected.rename(columns={'user/screen_name': 'user_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c9d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected['compound'] = df_selected['scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "df_selected['sentiment_type']=''\n",
    "df_selected.loc[df_selected.compound>0,'sentiment_type']='POSITIVE'\n",
    "df_selected.loc[df_selected.compound==0,'sentiment_type']='NEUTRAL'\n",
    "df_selected.loc[df_selected.compound<0,'sentiment_type']='NEGATIVE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab78d516",
   "metadata": {},
   "source": [
    "## **get hictorical price**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598791b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_prices(tickers, start_date, end_date):\n",
    "    data = {}\n",
    "    for ticker in tickers:\n",
    "        yahoo_financials = YahooFinancials(ticker)\n",
    "        historical_data = yahoo_financials.get_historical_price_data(start_date, end_date, \"daily\")\n",
    "        data[ticker] = historical_data[ticker]['prices']\n",
    "    dfs = []\n",
    "    for ticker, prices in data.items():\n",
    "        df = pd.DataFrame(prices)\n",
    "        df = df.drop('date', axis=1).set_index('formatted_date')\n",
    "        df.columns = [f\"{ticker}_close\", f\"{ticker}_high\", f\"{ticker}_low\", f\"{ticker}_open\", f\"{ticker}_volume\", f\"{ticker}_adjclose\"]\n",
    "        df['formatted_date'] = pd.to_datetime(df.index) # Add formatted_date column\n",
    "        dfs.append(df)\n",
    "    merged_df = pd.concat(dfs, axis=1)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b26f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['BTC-USD', 'ETH-USD', 'BNB-USD','XMR-USD','MATIC-USD','XRP-USD','DAI-USD','DOT-USD']\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2023-06-12'\n",
    "\n",
    "btc_df = get_historical_prices(tickers, start_date, end_date)\n",
    "# Move formatted_date column to the first position\n",
    "btc_df = btc_df[[\"formatted_date\"] + [col for col in btc_df.columns if col != \"formatted_date\"]]\n",
    "# Delete duplicate formatted_date columns\n",
    "btc_df = btc_df.loc[:, ~btc_df.columns.duplicated()]\n",
    "print(btc_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7fdb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare the data\n",
    "df_draw = btc_df[['formatted_date', 'XRP-USD_volume', 'ETH-USD_volume', 'BTC-USD_volume','XMR-USD_volume'\n",
    ",'DAI-USD_volume','DOT-USD_volume']]\n",
    "df_draw['formatted_date'] = pd.to_datetime(df_draw['formatted_date'], format='%Y-%m-%d %I-%p')\n",
    "df_draw.set_index('formatted_date', inplace=True)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))  # Set the figure size to 12 inches wide and 6 inches high\n",
    "plt.plot(df_draw.index, df_draw['XRP-USD_volume'], label='Ripple')\n",
    "plt.plot(df_draw.index, df_draw['ETH-USD_volume'], label='ETH')\n",
    "plt.plot(df_draw.index, df_draw['BTC-USD_volume'], label='BTC')\n",
    "plt.plot(df_draw.index, df_draw['XMR-USD_volume'], label='Monero')\n",
    "plt.plot(df_draw.index, df_draw['DOT-USD_volume'], label='Polkadot')\n",
    "#plt.plot(df_draw.index, df_draw['DAI-USD_volume'], label='Dai')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volume')\n",
    "plt.title('Cryptocurrency Volume Over Time')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020ac0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_selected = btc_df.iloc[:, :7]\n",
    "btc_selected = btc_selected.round(0).astype(int)\n",
    "btc_selected['formatted_date'] = btc_selected.index\n",
    "\n",
    "btc_selected.head()\n",
    "#---ETH-selecte------------\n",
    "eth_selected = btc_df.iloc[:, 7:13]\n",
    "eth_selected = eth_selected.round(0).astype(int)\n",
    "eth_selected['formatted_date'] = eth_selected.index\n",
    "\n",
    "eth_selected\n",
    "#--BNB-selected------------\n",
    "bnb_selected = btc_df.iloc[:, 13:19]\n",
    "bnb_selected = bnb_selected.round(0).astype(int)\n",
    "bnb_selected['formatted_date'] = bnb_selected.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9d60e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate price changes\n",
    "btc_selected['price_changes'] = btc_selected['BTC-USD_close'].diff()\n",
    "btc_selected['price_changes'] = btc_selected['price_changes'].apply(lambda x: 'positive' if x > 0 else 'negative' if x < 0 else 'neutral')\n",
    "\n",
    "# Calculate price changes\n",
    "eth_selected['price_changes'] = eth_selected['ETH-USD_close'].diff()\n",
    "eth_selected['price_changes'] = eth_selected['price_changes'].apply(lambda x: 'positive' if x > 0 else 'negative' if x < 0 else 'neutral')\n",
    "eth_selected\n",
    "# Calculate price changes\n",
    "bnb_selected['price_changes'] = bnb_selected['BNB-USD_close'].diff()\n",
    "bnb_selected['price_changes'] = bnb_selected['price_changes'].apply(lambda x: 'positive' if x > 0 else 'negative' if x < 0 else 'neutral')\n",
    "bnb_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6601cd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change columns name\n",
    "def remove_chars_before_underscore(df):\n",
    "    df.columns = df.columns.str.split('_').str[-1]\n",
    "remove_chars_before_underscore(btc_selected)\n",
    "remove_chars_before_underscore(eth_selected)\n",
    "remove_chars_before_underscore(bnb_selected)\n",
    "btc_selected\n",
    "eth_selected\n",
    "bnb_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b97626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add sentimeni type and compund to dataframe\n",
    "bnb_selected['date'] = pd.to_datetime(bnb_selected['date'])\n",
    "bnb_sentiment['created_at'] = pd.to_datetime(bnb_sentiment['created_at'])\n",
    "\n",
    "# Perform left merge on 'date' and 'created_at' columns\n",
    "bnb_selected = pd.merge(bnb_selected, bnb_sentiment[['created_at', 'compound', 'sentiment_type']],\n",
    "                     left_on='date', right_on='created_at', how='left')\n",
    "\n",
    "# Drop the redundant 'created_at' column\n",
    "bnb_selected = bnb_selected.drop('created_at', axis=1)\n",
    "\n",
    "bnb_selected\n",
    "\n",
    "## add sentimeni type and compund to dataframe\n",
    "eth_selected['date'] = pd.to_datetime(eth_selected['date'])\n",
    "eth_sentiment['created_at'] = pd.to_datetime(eth_sentiment['created_at'])\n",
    "\n",
    "# Perform left merge on 'date' and 'created_at' columns\n",
    "eth_selected = pd.merge(eth_selected, eth_sentiment[['created_at', 'compound', 'sentiment_type']],\n",
    "                     left_on='date', right_on='created_at', how='left')\n",
    "\n",
    "# Drop the redundant 'created_at' column\n",
    "eth_selected = eth_selected.drop('created_at', axis=1)\n",
    "\n",
    "eth_selected\n",
    "\n",
    "## add sentimeni type and compund to dataframe\n",
    "btc_selected['date'] = pd.to_datetime(btc_selected['date'])\n",
    "btc_sentiment['created_at'] = pd.to_datetime(btc_sentiment['created_at'])\n",
    "\n",
    "# Perform left merge on 'date' and 'created_at' columns\n",
    "btc_selected = pd.merge(btc_selected, btc_sentiment[['created_at', 'compound', 'sentiment_type']],\n",
    "                     left_on='date', right_on='created_at', how='left')\n",
    "\n",
    "# Drop the redundant 'created_at' column\n",
    "btc_selected = btc_selected.drop('created_at', axis=1)\n",
    "\n",
    "btc_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13152228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the top row\n",
    "btc_selected = btc_selected.iloc[1:]\n",
    "# Output the merged dataframe\n",
    "btc_selected\n",
    "\n",
    "\n",
    "# Drop the top row\n",
    "eth_selected = eth_selected.iloc[1:]\n",
    "# Output the merged dataframe\n",
    "eth_selected\n",
    "\n",
    "# Drop the top row\n",
    "bnb_selected = bnb_selected.iloc[1:]\n",
    "# Output the merged dataframe\n",
    "bnb_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb58b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eth_selected.to_csv('eth_selected_with_sentiment_2023_01_02_2023_06_12.csv')\n",
    "btc_selected.to_csv('btc_selected_with_sentiment_2023_01_02_2023_06_12.csv')\n",
    "bnb_selected.to_csv('bnb_selected_with_sentiment_2023_01_02_2023_06_12.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
